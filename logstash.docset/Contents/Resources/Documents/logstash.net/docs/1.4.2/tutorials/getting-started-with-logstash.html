<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="UTF-8">
  <title>logstash - open source log management</title>
  <link rel="stylesheet" href="../../..//style.css">
  </head>
  <body>
  <div class="container">
    <div class="header">
      <a href="http://logstash.net/.html"><img src="../../..//images/logo.png" alt="logstash"></a>
      <div class="nav">
        <a href="http://logstash.net/.html">home</a>
        <a href="/docs/1.4.2.html">docs</a>
        <a href="/docs/1.4.2/learn.html">about</a>
        <a href="https://github.com/elasticsearch/logstash/issues">bugs</a>
      </div>
    </div>
    <div id="content_right">
      <!--main content goes here, yo!-->
      <div class="content_wrapper">
        <img src="../../..//images/logstash.png" alt="logstash" class="mascot" style="float: right;">
         <h1>Introduction</h1>

<p>Logstash is a tool for receiving, processing and outputting logs. All
kinds of logs. System logs, webserver logs, error logs, application logs
and just about anything you can throw at it. Sounds great, eh?</p>

<p>Using Elasticsearch as a backend datastore, and kibana as a frontend
reporting tool, Logstash acts as the workhorse, creating a powerful
pipeline for storing, querying and analyzing your logs. With an arsenal
of built-in inputs, filters, codecs and outputs, you can harness some
powerful functionality with a small amount of effort. So, let’s get
started!</p>

<h2>Prerequisite: Java</h2>

<p>The only prerequisite required by Logstash is a Java runtime. You can
check that you have it installed by running the command <code>java -version</code>
in your shell. Here’s something similar to what you might see:</p>

<pre><code>&gt; java -version
java version "1.7.0_45"
Java(TM) SE Runtime Environment (build 1.7.0_45-b18)
Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode)
</code></pre>

<p>It is recommended to run a recent version of Java in order to ensure the
greatest success in running Logstash.</p>

<p>It’s fine to run an open-source version such as
OpenJDK:<a href="http://openjdk.java.net/">http://openjdk.java.net/</a></p>

<p>Or you can use the official Oracle
version:<a href="http://www.oracle.com/technetwork/java/index.html">http://www.oracle.com/technetwork/java/index.html</a></p>

<p>Once you have verified the existence of Java on your system, we can move
on!</p>

<h1>Up and Running!</h1>

<h2>Logstash in two commands</h2>

<p>First, we’re going to download the <em>logstash</em> binary and run it with a
very simple configuration.</p>

<pre><code>curl -O https://download.elasticsearch.org/logstash/logstash/logstash-1.4.2.tar.gz
</code></pre>

<p>Now you should have the file named <em>logstash-1.4.2.tar.gz</em> on your
local filesystem. Let’s unpack it:</p>

<pre><code>tar zxvf logstash-1.4.2.tar.gz
cd logstash-1.4.2
</code></pre>

<p>Now let’s run it:</p>

<pre><code>bin/logstash -e 'input { stdin { } } output { stdout {} }'
</code></pre>

<p>Now type something into your command prompt, and you will see it output
by Logstash:</p>

<pre><code>hello world
2013-11-21T01:22:14.405+0000 0.0.0.0 hello world
</code></pre>

<p>OK, that’s interesting… We ran Logstash with an input called "stdin",
and an output named "stdout", and Logstash basically echoed back
whatever we typed in some sort of structured format. Note that
specifying the <strong>-e</strong> command line flag allows Logstash to accept a
configuration directly from the command line. This is especially useful
for quickly testing configurations without having to edit a file between
iterations.</p>

<p>Let’s try a slightly fancier example. First, you should exit Logstash by
issuing a <em>CTRL-C</em> command in the shell in which it is running. Now run
Logstash again with the following command:</p>

<pre><code>bin/logstash -e 'input { stdin { } } output { stdout { codec =&gt; rubydebug } }'
</code></pre>

<p>And then try another test input, typing the text "goodnight moon":</p>

<pre><code>goodnight moon
{
  "message" =&gt; "goodnight moon",
  "@timestamp" =&gt; "2013-11-20T23:48:05.335Z",
  "@version" =&gt; "1",
  "host" =&gt; "my-laptop"
}
</code></pre>

<p>So, by re-configuring the "stdout" output (adding a "codec"), we can
change the output of Logstash. By adding inputs, outputs and filters to
your configuration, it’s possible to massage the log data in many ways,
in order to maximize flexibility of the stored data when you are
querying it.</p>

<h1>Storing logs with Elasticsearch</h1>

<p>Now, you’re probably saying, "that’s all fine and dandy, but typing all
my logs into Logstash isn’t really an option, and merely seeing them
spit to STDOUT isn’t very useful." Good point. First, let’s set up
Elasticsearch to store the messages we send into Logstash. If you don’t
have Elasticearch already installed, you can <a href="http://www.elasticsearch.org/download/">download the RPM or DEB
package</a>, or install manually by
downloading the current release tarball, by issuing the following four
commands:</p>

<pre><code>curl -O https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.1.1.tar.gz
tar zxvf elasticsearch-1.1.1.tar.gz
cd elasticsearch-1.1.1/
./bin/elasticsearch
</code></pre>

<blockquote>
<p><strong>Note</strong></p>

<p>This tutorial specifies running Logstash 1.4.2 with Elasticsearch
1.1.1. Each release of Logstash has a
<strong>recommended</strong> version of Elasticsearch to pair with. Make sure the
versions match based on the <a href="http://logstash.net/docs/latest">Logstash
version</a> you’re running!</p>
</blockquote>

<p>More detailed information on installing and configuring Elasticsearch
can be found on <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index.html">The Elasticsearch reference
pages</a>.
However, for the purposes of Getting Started with Logstash, the default
installation and configuration of Elasticsearch should be sufficient.</p>

<p>Now that we have Elasticsearch running on port 9200 (we do, right?),
Logstash can be simply configured to use Elasticsearch as its backend.
The defaults for both Logstash and Elasticsearch are fairly sane and
well thought out, so we can omit the optional configurations within the
elasticsearch output:</p>

<pre><code>bin/logstash -e 'input { stdin { } } output { elasticsearch { host =&gt; localhost } }'
</code></pre>

<p>Type something, and Logstash will process it as before (this time you
won’t see any output, since we don’t have the stdout output configured)</p>

<pre><code>you know, for logs
</code></pre>

<p>You can confirm that ES actually received the data by making a curl
request and inspecting the return:</p>

<pre><code>curl 'http://localhost:9200/_search?pretty'
</code></pre>

<p>which should return something like this:</p>

<pre><code>{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "logstash-2013.11.21",
      "_type" : "logs",
      "_id" : "2ijaoKqARqGvbMgP3BspJA",
      "_score" : 1.0, "_source" : {"message":"you know, for logs","@timestamp":"2013-11-21T18:45:09.862Z","@version":"1","host":"my-laptop"}
    } ]
  }
}
</code></pre>

<p>Congratulations! You’ve successfully stashed logs in Elasticsearch via
Logstash.</p>

<h2>Elasticsearch Plugins (an aside)</h2>

<p>Another very useful tool for querying your Logstash data (and
Elasticsearch in general) is the Elasticearch-kopf plugin. Here is more
information on <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-plugins.html">Elasticsearch
plugins</a>.
To install elasticsearch-kopf, simply issue the following command in
your Elasticsearch directory (the same one in which you ran
Elasticsearch earlier):</p>

<pre><code>bin/plugin -install lmenezes/elasticsearch-kopf
</code></pre>

<p>Now you can browse to <a href="http://localhost:9200/_plugin/kopf">http://localhost:9200/_plugin/kopf</a> to browse
your Elasticsearch data, settings and mappings!</p>

<h2>Multiple Outputs</h2>

<p>As a quick exercise in configuring multiple Logstash outputs, let’s
invoke Logstash again, using both the <em>stdout</em> as well as the
<em>elasticsearch</em> output:</p>

<pre><code>bin/logstash -e 'input { stdin { } } output { elasticsearch { host =&gt; localhost } stdout { } }'
</code></pre>

<p>Typing a phrase will now echo back to your terminal, as well as save in
Elasticsearch! (Feel free to verify this using curl or
elasticsearch-kopf).</p>

<h2>Default - Daily Indices</h2>

<p>You might notice that Logstash was smart enough to create a new index in
Elasticsearch… The default index name is in the form of
<em>logstash-YYYY.MM.DD</em>, which essentially creates one index per day. At
midnight (GMT?), Logstash will automagically rotate the index to a fresh
new one, with the new current day’s timestamp. This allows you to keep
windows of data, based on how far retroactively you’d like to query your
log data. Of course, you can always archive (or re-index) your data to
an alternate location, where you are able to query further into the
past. If you’d like to simply delete old indices after a certain time
period, you can use the <a href="https://github.com/elasticsearch/curator">Elasticsearch Curator
tool</a>.</p>

<h1>Moving On</h1>

<p>Now you’re ready for more advanced configurations. At this point, it
makes sense for a quick discussion of some of the core features of
Logstash, and how they interact with the Logstash engine.</p>

<h2>The Life of an Event</h2>

<p>Inputs, Outputs, Codecs and Filters are at the heart of the Logstash
configuration. By creating a pipeline of event processing, Logstash is
able to extract the relevant data from your logs and make it available
to elasticsearch, in order to efficiently query your data. To get you
thinking about the various options available in Logstash, let’s discuss
some of the more common configurations currently in use. For more
details, read about <a href="http://logstash.net/docs/latest/life-of-an-event">the Logstash event
pipeline</a>.</p>

<h3>Inputs</h3>

<p>Inputs are the mechanism for passing log data to Logstash. Some of the
more useful, commonly-used ones are:</p>

<ul>
<li><p><strong>file</strong>: reads from a file on the filesystem, much like the UNIX
command "tail -0a"</p></li>
<li><p><strong>syslog</strong>: listens on the well-known port 514 for syslog messages
and parses according to RFC3164 format</p></li>
<li><p><strong>redis</strong>: reads from a redis server, using both redis channels and
also redis lists. Redis is often used as a "broker" in a centralized
Logstash installation, which queues Logstash events from remote
Logstash "shippers".</p></li>
<li><p><strong>lumberjack</strong>: processes events sent in the lumberjack protocol.
Now called
<a href="https://github.com/elasticsearch/logstash-forwarder">logstash-forwarder</a>.</p></li>
</ul>


<h3>Filters</h3>

<p>Filters are used as intermediary processing devices in the Logstash
chain. They are often combined with conditionals in order to perform a
certain action on an event, if it matches particular criteria. Some
useful filters:</p>

<ul>
<li><p><strong>grok</strong>: parses arbitrary text and structure it. Grok is currently
the best way in Logstash to parse unstructured log data into
something structured and queryable. With 120 patterns shipped
built-in to Logstash, it’s more than likely you’ll find one that
meets your needs!</p></li>
<li><p><strong>mutate</strong>: The mutate filter allows you to do general mutations to
fields. You can rename, remove, replace, and modify fields in your
events.</p></li>
<li><p><strong>drop</strong>: drop an event completely, for example, <em>debug</em> events.</p></li>
<li><p><strong>clone</strong>: make a copy of an event, possibly adding or removing
fields.</p></li>
<li><p><strong>geoip</strong>: adds information about geographical location of IP
addresses (and displays amazing charts in kibana)</p></li>
</ul>


<h3>Outputs</h3>

<p>Outputs are the final phase of the Logstash pipeline. An event may pass
through multiple outputs during processing, but once all outputs are
complete, the event has finished its execution. Some commonly used
outputs include:</p>

<ul>
<li><p><strong>elasticsearch</strong>: If you’re planning to save your data in an
efficient, convenient and easily queryable format… Elasticsearch is
the way to go. Period. Yes, we’re biased :)</p></li>
<li><p><strong>file</strong>: writes event data to a file on disk.</p></li>
<li><p><strong>graphite</strong>: sends event data to graphite, a popular open source
tool for storing and graphing metrics.
<a href="http://graphite.wikidot.com/">http://graphite.wikidot.com/</a></p></li>
<li><p><strong>statsd</strong>: a service which "listens for statistics, like counters
and timers, sent over UDP and sends aggregates to one or more
pluggable backend services". If you’re already using statsd, this
could be useful for you!</p></li>
</ul>


<h3>Codecs</h3>

<p>Codecs are basically stream filters which can operate as part of an
input, or an output. Codecs allow you to easily separate the transport
of your messages from the serialization process. Popular codecs include
<em>json</em>, <em>msgpack</em> and <em>plain</em> (text).</p>

<ul>
<li><p><strong>json</strong>: encode / decode data in JSON format</p></li>
<li><p><strong>multiline</strong>: Takes multiple-line text events and merge them into a
single event, e.g. java exception and stacktrace messages</p></li>
</ul>


<p>For the complete list of (current) configurations, visit the Logstash
"plugin configuration" section of the <a href="http://logstash.net/docs/latest/">Logstash documentation
page</a>.</p>

<h1>More fun with Logstash</h1>

<h2>Persistent Configuration files</h2>

<p>Specifying configurations on the command line using <em>-e</em> is only so
helpful, and more advanced setups will require more lengthy, long-lived
configurations. First, let’s create a simple configuration file, and
invoke Logstash using it. Create a file named "logstash-simple.conf" and
save it in the same directory as Logstash.</p>

<pre><code>input { stdin { } }
output {
  elasticsearch { host =&gt; localhost }
  stdout { codec =&gt; rubydebug }
}
</code></pre>

<p>Then, run this command:</p>

<pre><code>bin/logstash -f logstash-simple.conf
</code></pre>

<p>Et voilà! Logstash will read in the configuration file you just created
and run as in the example we saw earlier. Note that we used the <em>-f</em> to
read in the file, rather than the <em>-e</em> to read the configuration from
the command line. This is a very simple case, of course, so let’s move
on to some more complex examples.</p>

<h2>Filters</h2>

<p>Filters are an in-line processing mechanism which provide the
flexibility to slice and dice your data to fit your needs. Let’s see one
in action, namely the <strong>grok filter</strong>.</p>

<pre><code>input { stdin { } }

filter {
  grok {
    match =&gt; { "message" =&gt; "%{COMBINEDAPACHELOG}" }
  }
  date {
    match =&gt; [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
  }
}

output {
  elasticsearch { host =&gt; localhost }
  stdout { codec =&gt; rubydebug }
}
</code></pre>

<p>Run Logstash with this configuration:</p>

<pre><code>bin/logstash -f logstash-filter.conf
</code></pre>

<p>Now paste this line into the terminal (so it will be processed by the
stdin input):</p>

<pre><code>127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] "GET /xampp/status.php HTTP/1.1" 200 3891 "http://cadenza/xampp/navi.php" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0"
</code></pre>

<p>You should see something returned to STDOUT which looks like this:</p>

<pre><code>{
        "message" =&gt; "127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] \"GET /xampp/status.php HTTP/1.1\" 200 3891 \"http://cadenza/xampp/navi.php\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\"",
     "@timestamp" =&gt; "2013-12-11T08:01:45.000Z",
       "@version" =&gt; "1",
           "host" =&gt; "cadenza",
       "clientip" =&gt; "127.0.0.1",
          "ident" =&gt; "-",
           "auth" =&gt; "-",
      "timestamp" =&gt; "11/Dec/2013:00:01:45 -0800",
           "verb" =&gt; "GET",
        "request" =&gt; "/xampp/status.php",
    "httpversion" =&gt; "1.1",
       "response" =&gt; "200",
          "bytes" =&gt; "3891",
       "referrer" =&gt; "\"http://cadenza/xampp/navi.php\"",
          "agent" =&gt; "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\""
}
</code></pre>

<p>As you can see, Logstash (with help from the <strong>grok</strong> filter) was able
to parse the log line (which happens to be in Apache "combined log"
format) and break it up into many different discrete bits of
information. This will be extremely useful later when we start querying
and analyzing our log data… for example, we’ll be able to run reports on
HTTP response codes, IP addresses, referrers, etc. very easily. There
are quite a few grok patterns included with Logstash out-of-the-box, so
it’s quite likely if you’re attempting to parse a fairly common log
format, someone has already done the work for you. For more details, see
the list of <a href="https://github.com/logstash/logstash/blob/master/patterns/grok-patterns">logstash grok
patterns</a>
on github.</p>

<p>The other filter used in this example is the <strong>date</strong> filter. This
filter parses out a timestamp and uses it as the timestamp for the event
(regardless of when you’re ingesting the log data). You’ll notice that
the @timestamp field in this example is set to December 11, 2013, even
though Logstash is ingesting the event at some point afterwards. This is
handy when backfilling logs, for example… the ability to tell Logstash
"use this value as the timestamp for this event".</p>

<h1>Useful Examples</h1>

<h2>Apache logs (from files)</h2>

<p>Now, let’s configure something actually <strong>useful</strong>… apache2 access log
files! We are going to read the input from a file on the localhost, and
use a <strong>conditional</strong> to process the event according to our needs.
First, create a file called something like <em>logstash-apache.conf</em> with
the following contents (you’ll need to change the log’s file path to
suit your needs):</p>

<pre><code>input {
  file {
    path =&gt; "/tmp/access_log"
    start_position =&gt; beginning
  }
}

filter {
  if [path] =~ "access" {
    mutate { replace =&gt; { "type" =&gt; "apache_access" } }
    grok {
      match =&gt; { "message" =&gt; "%{COMBINEDAPACHELOG}" }
    }
  }
  date {
    match =&gt; [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
  }
}

output {
  elasticsearch {
    host =&gt; localhost
  }
  stdout { codec =&gt; rubydebug }
}
</code></pre>

<p>Then, create the file you configured above (in this example,
"/tmp/access_log") with the following log lines as contents (or use
some from your own webserver):</p>

<pre><code>71.141.244.242 - kurt [18/May/2011:01:48:10 -0700] "GET /admin HTTP/1.1" 301 566 "-" "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.2.3) Gecko/20100401 Firefox/3.6.3"
134.39.72.245 - - [18/May/2011:12:40:18 -0700] "GET /favicon.ico HTTP/1.1" 200 1189 "-" "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; InfoPath.2; .NET4.0C; .NET4.0E)"
98.83.179.51 - - [18/May/2011:19:35:08 -0700] "GET /css/main.css HTTP/1.1" 200 1837 "http://www.safesand.com/information.htm" "Mozilla/5.0 (Windows NT 6.0; WOW64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1"
</code></pre>

<p>Now run it with the -f flag as in the last example:</p>

<pre><code>bin/logstash -f logstash-apache.conf
</code></pre>

<p>You should be able to see your apache log data in Elasticsearch now!
You’ll notice that Logstash opened the file you configured, and read
through it, processing any events it encountered. Any additional lines
logged to this file will also be captured, processed by Logstash as
events and stored in Elasticsearch. As an added bonus, they will be
stashed with the field "type" set to "apache_access" (this is done by
the type ⇒ "apache_access" line in the input configuration).</p>

<p>In this configuration, Logstash is only watching the apache access_log,
but it’s easy enough to watch both the access_log and the error_log
(actually, any file matching <em>*log</em>), by changing one line in the above
configuration, like this:</p>

<pre><code>input {
  file {
    path =&gt; "/tmp/*_log"
...
</code></pre>

<p>Now, rerun Logstash, and you will see both the error and access logs
processed via Logstash. However, if you inspect your data (using
elasticsearch-kopf, perhaps), you will see that the access_log was
broken up into discrete fields, but not the error_log. That’s because
we used a "grok" filter to match the standard combined apache log format
and automatically split the data into separate fields. Wouldn’t it be
nice <strong>if</strong> we could control how a line was parsed, based on its format?
Well, we can…</p>

<p>Also, you might have noticed that Logstash did not reprocess the events
which were already seen in the access_log file. Logstash is able to
save its position in files, only processing new lines as they are added
to the file. Neat!</p>

<h2>Conditionals</h2>

<p>Now we can build on the previous example, where we introduced the
concept of a <strong>conditional</strong>. A conditional should be familiar to most
Logstash users, in the general sense. You may use <em>if</em>, <em>else if</em> and
<em>else</em> statements, as in many other programming languages. Let’s label
each event according to which file it appeared in (access_log,
error_log and other random files which end with "log").</p>

<pre><code>input {
  file {
    path =&gt; "/tmp/*_log"
  }
}

filter {
  if [path] =~ "access" {
    mutate { replace =&gt; { type =&gt; "apache_access" } }
    grok {
      match =&gt; { "message" =&gt; "%{COMBINEDAPACHELOG}" }
    }
    date {
      match =&gt; [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
  } else if [path] =~ "error" {
    mutate { replace =&gt; { type =&gt; "apache_error" } }
  } else {
    mutate { replace =&gt; { type =&gt; "random_logs" } }
  }
}

output {
  elasticsearch { host =&gt; localhost }
  stdout { codec =&gt; rubydebug }
}
</code></pre>

<p>You’ll notice we’ve labeled all events using the "type" field, but we
didn’t actually parse the "error" or "random" files… There are so many
types of error logs that it’s better left as an exercise for you,
depending on the logs you’re seeing.</p>

<h2>Syslog</h2>

<p>OK, now we can move on to another incredibly useful example: <strong>syslog</strong>.
Syslog is one of the most common use cases for Logstash, and one it
handles exceedingly well (as long as the log lines conform roughly to
RFC3164 :). Syslog is the de facto UNIX networked logging standard,
sending messages from client machines to a local file, or to a
centralized log server via rsyslog. For this example, you won’t need a
functioning syslog instance; we’ll fake it from the command line, so you
can get a feel for what happens.</p>

<p>First, let’s make a simple configuration file for Logstash + syslog,
called <em>logstash-syslog.conf</em>.</p>

<pre><code>input {
  tcp {
    port =&gt; 5000
    type =&gt; syslog
  }
  udp {
    port =&gt; 5000
    type =&gt; syslog
  }
}

filter {
  if [type] == "syslog" {
    grok {
      match =&gt; { "message" =&gt; "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
      add_field =&gt; [ "received_at", "%{@timestamp}" ]
      add_field =&gt; [ "received_from", "%{host}" ]
    }
    syslog_pri { }
    date {
      match =&gt; [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }
  }
}

output {
  elasticsearch { host =&gt; localhost }
  stdout { codec =&gt; rubydebug }
}
</code></pre>

<p>Run it as normal:</p>

<pre><code>bin/logstash -f logstash-syslog.conf
</code></pre>

<p>Normally, a client machine would connect to the Logstash instance on
port 5000 and send its message. In this simplified case, we’re simply
going to telnet to Logstash and enter a log line (similar to how we
entered log lines into STDIN earlier). First, open another shell window
to interact with the Logstash syslog input and type the following
command:</p>

<pre><code>telnet localhost 5000
</code></pre>

<p>You can copy and paste the following lines as samples (feel free to try
some of your own, but keep in mind they might not parse if the grok
filter is not correct for your data):</p>

<pre><code>Dec 23 12:11:43 louis postfix/smtpd[31499]: connect from unknown[95.75.93.154]
Dec 23 14:42:56 louis named[16000]: client 199.48.164.7#64817: query (cache) 'amsterdamboothuren.com/MX/IN' denied
Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php &gt;/dev/null 2&gt;/var/log/cacti/poller-error.log)
Dec 22 18:28:06 louis rsyslogd: [origin software="rsyslogd" swVersion="4.2.0" x-pid="2253" x-info="http://www.rsyslog.com"] rsyslogd was HUPed, type 'lightweight'.
</code></pre>

<p>Now you should see the output of Logstash in your original shell as it
processes and parses messages!</p>

<pre><code>{
                 "message" =&gt; "Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php &gt;/dev/null 2&gt;/var/log/cacti/poller-error.log)",
              "@timestamp" =&gt; "2013-12-23T22:30:01.000Z",
                "@version" =&gt; "1",
                    "type" =&gt; "syslog",
                    "host" =&gt; "0:0:0:0:0:0:0:1:52617",
        "syslog_timestamp" =&gt; "Dec 23 14:30:01",
         "syslog_hostname" =&gt; "louis",
          "syslog_program" =&gt; "CRON",
              "syslog_pid" =&gt; "619",
          "syslog_message" =&gt; "(www-data) CMD (php /usr/share/cacti/site/poller.php &gt;/dev/null 2&gt;/var/log/cacti/poller-error.log)",
             "received_at" =&gt; "2013-12-23 22:49:22 UTC",
           "received_from" =&gt; "0:0:0:0:0:0:0:1:52617",
    "syslog_severity_code" =&gt; 5,
    "syslog_facility_code" =&gt; 1,
         "syslog_facility" =&gt; "user-level",
         "syslog_severity" =&gt; "notice"
}
</code></pre>

<p>Congratulations! You’re well on your way to being a real Logstash power
user. You should be comfortable configuring, running and sending events
to Logstash, but there’s much more to explore.</p>

      </div>
      <div class="clear">
      </div>
    </div>
  </div>
  <!--closes main container div-->
  <div class="clear">
  </div>
  <div class="footer">
    <p>
      Hello! I'm your friendly footer. If you're actually reading this, I'm impressed.
    </p>
  </div>
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-6522917-5']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  <script type="text/javascript">
/* <![CDATA[ */
  var google_conversion_id = 985891458;
  var google_custom_params = window.google_tag_params;
  var google_remarketing_only = true;
  /* ]]> */
  </script>
  <script type="text/javascript" src="//www.googleadservices.com/pagead/conversion.js">
  </script>
  <noscript>
    <div style="display:inline;">
      <img height="1" width="1" style="border-style:none;" alt="" src="//googleads.g.doubleclick.net/pagead/viewthroughconversion/985891458/?value=0&amp;guid=ON&amp;script=0">
    </div>
  </noscript>
  <script src="/js/patch.js?1.4.2"></script>
  </body>
</html>
